#!/usr/bin/env python3
"""
–ì–ª—É–±–æ–∫–∞—è –∏–Ω—Å–ø–µ–∫—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è—Å–Ω–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
–∏ –ø—Ä–∏—á–∏–Ω –±–æ–ª—å—à–æ–≥–æ –æ–±—ä–µ–º–∞
"""

import os
import json
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
from collections import defaultdict
import subprocess

class DeepDataInspector:
    def __init__(self, minio_path="data/minio"):
        self.minio_path = Path(minio_path)
        self.warehouse_path = self.minio_path / "warehouse"
        
    def full_inspection(self):
        """–ü–æ–ª–Ω–∞—è –∏–Ω—Å–ø–µ–∫—Ü–∏—è - –ß–¢–û –∏–º–µ–Ω–Ω–æ –∑–∞–Ω–∏–º–∞–µ—Ç –º–µ—Å—Ç–æ"""
        print("üîç –ì–õ–£–ë–û–ö–ê–Ø –ò–ù–°–ü–ï–ö–¶–ò–Ø –î–ê–ù–ù–´–•")
        print("="*80)
        
        # 1. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
        print("\n1Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –†–ê–ó–ú–ï–†–û–í –ü–û –î–ò–†–ï–ö–¢–û–†–ò–Ø–ú")
        dir_sizes = self.analyze_directory_sizes()
        
        # 2. –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–æ–≤
        print("\n2Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –°–û–î–ï–†–ñ–ò–ú–û–ì–û –§–ê–ô–õ–û–í")
        content_analysis = self.analyze_file_contents()
        
        # 3. –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏
        print("\n3Ô∏è‚É£ –ü–û–ò–°–ö –î–£–ë–õ–ò–ö–ê–¢–û–í –ò –ò–ó–ë–´–¢–û–ß–ù–û–°–¢–ò")
        duplicates = self.find_data_redundancy()
        
        # 4. –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        print("\n4Ô∏è‚É£ –í–†–ï–ú–ï–ù–ù–û–ï –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•")
        time_analysis = self.analyze_time_distribution()
        
        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
        print("\n5Ô∏è‚É£ –ü–û–ò–°–ö –ê–ù–û–ú–ê–õ–ò–ô –í –î–ê–ù–ù–´–•")
        anomalies = self.find_data_anomalies()
        
        # 6. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        self.print_inspection_report({
            'dir_sizes': dir_sizes,
            'content': content_analysis,
            'duplicates': duplicates,
            'time_analysis': time_analysis,
            'anomalies': anomalies
        })
    
    def analyze_directory_sizes(self):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        print("   –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
        
        sizes = {}
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—É—é –∫–æ–º–∞–Ω–¥—É du –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞
        try:
            result = subprocess.run(['du', '-h', '--max-depth=3', str(self.warehouse_path)], 
                                  capture_output=True, text=True)
            
            for line in result.stdout.strip().split('\n'):
                if line:
                    parts = line.split('\t')
                    if len(parts) == 2:
                        size_str, path = parts
                        relative_path = Path(path).relative_to(self.warehouse_path)
                        sizes[str(relative_path)] = size_str
                        
        except Exception as e:
            print(f"   –û—à–∏–±–∫–∞ du –∫–æ–º–∞–Ω–¥—ã: {e}")
            
            # Fallback - —Ä—É—á–Ω–æ–π –ø–æ–¥—Å—á–µ—Ç
            for root, dirs, files in os.walk(self.warehouse_path):
                total_size = 0
                for file in files:
                    try:
                        file_path = Path(root) / file
                        total_size += file_path.stat().st_size
                    except:
                        pass
                
                if total_size > 0:
                    relative_path = Path(root).relative_to(self.warehouse_path)
                    sizes[str(relative_path)] = self._format_size(total_size)
        
        return sizes
    
    def analyze_file_contents(self):
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–æ–≤"""
        print("   –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤...")
        
        analysis = {
            'file_types': {},
            'largest_files': [],
            'sample_contents': [],
            'compression_efficiency': {}
        }
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        all_files = []
        for root, dirs, files in os.walk(self.warehouse_path):
            for file in files:
                file_path = Path(root) / file
                try:
                    size = file_path.stat().st_size
                    ext = file_path.suffix.lower()
                    
                    all_files.append({
                        'path': file_path,
                        'size': size,
                        'extension': ext,
                        'name': file
                    })
                except:
                    pass
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
        by_extension = defaultdict(list)
        for file_info in all_files:
            by_extension[file_info['extension']].append(file_info)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–ø
        for ext, files in by_extension.items():
            total_size = sum(f['size'] for f in files)
            avg_size = total_size / len(files)
            
            analysis['file_types'][ext] = {
                'count': len(files),
                'total_size': total_size,
                'avg_size': avg_size,
                'largest': max(files, key=lambda x: x['size']) if files else None
            }
        
        # –¢–æ–ø-20 —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        all_files.sort(key=lambda x: x['size'], reverse=True)
        analysis['largest_files'] = all_files[:20]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤
        sample_files = all_files[:10]  # –¢–æ–ø-10 —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö
        
        for file_info in sample_files:
            file_path = file_info['path']
            content_info = self._analyze_single_file(file_path)
            if content_info:
                analysis['sample_contents'].append({
                    'file': str(file_path),
                    'size': file_info['size'],
                    'content': content_info
                })
        
        return analysis
    
    def _analyze_single_file(self, file_path):
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            size = file_path.stat().st_size
            ext = file_path.suffix.lower()
            
            content_info = {
                'size': size,
                'extension': ext,
                'readable': False,
                'type': 'unknown'
            }
            
            # –ü–æ–ø—ã—Ç–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Ñ–∞–π–ª–∞
            if ext in ['.1', '.parquet']:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª Parquet
                try:
                    with open(file_path, 'rb') as f:
                        header = f.read(4)
                        if header == b'PAR1':
                            content_info['type'] = 'parquet'
                            content_info['readable'] = True
                            
                            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ Parquet
                            import pyarrow.parquet as pq
                            parquet_file = pq.ParquetFile(file_path)
                            metadata = parquet_file.metadata
                            
                            content_info['parquet_info'] = {
                                'num_rows': metadata.num_rows,
                                'num_columns': metadata.num_columns,
                                'num_row_groups': metadata.num_row_groups,
                                'compressed_size': metadata.serialized_size,
                                'schema': str(parquet_file.schema)[:200] + "..."
                            }
                            
                        else:
                            content_info['type'] = 'binary_unknown'
                            content_info['header_hex'] = header.hex()
                except Exception as e:
                    content_info['read_error'] = str(e)
            
            elif ext == '.meta':
                # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞-—Ñ–∞–π–ª–æ–≤
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        content_info['type'] = 'text'
                        content_info['readable'] = True
                        content_info['char_count'] = len(content)
                        content_info['line_count'] = content.count('\n')
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                        try:
                            json_data = json.loads(content)
                            content_info['json_valid'] = True
                            content_info['json_keys'] = list(json_data.keys()) if isinstance(json_data, dict) else None
                        except:
                            content_info['json_valid'] = False
                            
                        content_info['preview'] = content[:200] + "..." if len(content) > 200 else content
                        
                except Exception as e:
                    content_info['read_error'] = str(e)
            
            elif ext == '.json':
                # JSON —Ñ–∞–π–ª—ã
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        json_data = json.loads(content)
                        
                        content_info.update({
                            'type': 'json',
                            'readable': True,
                            'char_count': len(content),
                            'json_structure': type(json_data).__name__,
                            'preview': str(json_data)[:200] + "..."
                        })
                        
                        if isinstance(json_data, dict):
                            content_info['json_keys'] = list(json_data.keys())
                            
                except Exception as e:
                    content_info['read_error'] = str(e)
            
            return content_info
            
        except Exception as e:
            return {'error': str(e)}
    
    def find_data_redundancy(self):
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        print("   –ò—â–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å...")
        
        redundancy = {
            'identical_files': [],
            'similar_files': [],
            'empty_files': [],
            'suspicious_patterns': []
        }
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        size_groups = defaultdict(list)
        
        for root, dirs, files in os.walk(self.warehouse_path):
            for file in files:
                file_path = Path(root) / file
                try:
                    size = file_path.stat().st_size
                    
                    if size == 0:
                        redundancy['empty_files'].append(str(file_path))
                    else:
                        size_groups[size].append(file_path)
                        
                except:
                    pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä—É–ø–ø—ã –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        for size, files in size_groups.items():
            if len(files) > 1:
                # –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                if size < 1024 * 1024:  # < 1MB
                    content_groups = defaultdict(list)
                    
                    for file_path in files:
                        try:
                            with open(file_path, 'rb') as f:
                                content = f.read()
                                content_hash = hash(content)
                                content_groups[content_hash].append(file_path)
                        except:
                            pass
                    
                    for content_hash, identical_files in content_groups.items():
                        if len(identical_files) > 1:
                            redundancy['identical_files'].append({
                                'files': [str(f) for f in identical_files],
                                'size': size,
                                'count': len(identical_files),
                                'waste_space': size * (len(identical_files) - 1)
                            })
                else:
                    # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ
                    redundancy['similar_files'].append({
                        'files': [str(f) for f in files],
                        'size': size,
                        'count': len(files)
                    })
        
        return redundancy
    
    def analyze_time_distribution(self):
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        print("   –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ...")
        
        time_dist = {
            'files_by_hour': defaultdict(int),
            'files_by_day': defaultdict(int),
            'oldest_file': None,
            'newest_file': None,
            'time_gaps': []
        }
        
        file_times = []
        
        for root, dirs, files in os.walk(self.warehouse_path):
            for file in files:
                file_path = Path(root) / file
                try:
                    stat = file_path.stat()
                    mod_time = datetime.fromtimestamp(stat.st_mtime)
                    
                    file_times.append({
                        'path': file_path,
                        'time': mod_time,
                        'size': stat.st_size
                    })
                    
                    time_dist['files_by_hour'][mod_time.hour] += 1
                    time_dist['files_by_day'][mod_time.date()] += 1
                    
                except:
                    pass
        
        if file_times:
            file_times.sort(key=lambda x: x['time'])
            time_dist['oldest_file'] = file_times[0]
            time_dist['newest_file'] = file_times[-1]
            
            # –ò—â–µ–º –±–æ–ª—å—à–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–∑—Ä—ã–≤—ã
            for i in range(1, len(file_times)):
                gap = (file_times[i]['time'] - file_times[i-1]['time']).total_seconds()
                if gap > 3600:  # –ë–æ–ª—å—à–µ —á–∞—Å–∞
                    time_dist['time_gaps'].append({
                        'start': file_times[i-1]['time'],
                        'end': file_times[i]['time'],
                        'gap_hours': gap / 3600
                    })
        
        return time_dist
    
    def find_data_anomalies(self):
        """–ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö"""
        print("   –ò—â–µ–º –∞–Ω–æ–º–∞–ª–∏–∏...")
        
        anomalies = {
            'oversized_files': [],
            'undersized_files': [],
            'suspicious_names': [],
            'encoding_issues': []
        }
        
        file_sizes = []
        
        for root, dirs, files in os.walk(self.warehouse_path):
            for file in files:
                file_path = Path(root) / file
                try:
                    size = file_path.stat().st_size
                    file_sizes.append(size)
                    
                    ext = file_path.suffix.lower()
                    
                    # –ê–Ω–æ–º–∞–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞
                    if ext == '.meta' and size > 100 * 1024:  # > 100KB –¥–ª—è meta
                        anomalies['oversized_files'].append({
                            'file': str(file_path),
                            'size': size,
                            'type': 'meta',
                            'expected_max': '10KB'
                        })
                    
                    elif ext in ['.1', '.parquet'] and size > 500 * 1024 * 1024:  # > 500MB
                        anomalies['oversized_files'].append({
                            'file': str(file_path),
                            'size': size,
                            'type': 'data',
                            'expected_max': '100MB'
                        })
                    
                    # –ê–Ω–æ–º–∞–ª—å–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã
                    elif ext in ['.1', '.parquet'] and size < 1024:  # < 1KB
                        anomalies['undersized_files'].append({
                            'file': str(file_path),
                            'size': size,
                            'type': 'data'
                        })
                    
                    # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞
                    if 'temp' in file.lower() or 'tmp' in file.lower():
                        anomalies['suspicious_names'].append(str(file_path))
                    
                except:
                    pass
        
        return anomalies
    
    def _format_size(self, size_bytes):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"
    
    def print_inspection_report(self, results):
        """–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –∏–Ω—Å–ø–µ–∫—Ü–∏–∏"""
        print("\n" + "="*80)
        print("üìã –û–¢–ß–ï–¢ –ì–õ–£–ë–û–ö–û–ô –ò–ù–°–ü–ï–ö–¶–ò–ò")
        print("="*80)
        
        # 1. –†–∞–∑–º–µ—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        print(f"\nüìÅ –†–ê–ó–ú–ï–†–´ –î–ò–†–ï–ö–¢–û–†–ò–ô (–¢–û–ü-20):")
        dir_sizes = results['dir_sizes']
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É (–ø—Ä–∏–º–µ—Ä–Ω–æ)
        sorted_dirs = []
        for path, size_str in dir_sizes.items():
            # –ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –≤ –±–∞–π—Ç—ã –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            size_multipliers = {'K': 1024, 'M': 1024**2, 'G': 1024**3, 'T': 1024**4}
            try:
                if size_str[-1] in size_multipliers:
                    size_bytes = float(size_str[:-1]) * size_multipliers[size_str[-1]]
                else:
                    size_bytes = float(size_str)
                sorted_dirs.append((path, size_str, size_bytes))
            except:
                sorted_dirs.append((path, size_str, 0))
        
        sorted_dirs.sort(key=lambda x: x[2], reverse=True)
        
        for path, size_str, _ in sorted_dirs[:20]:
            print(f"   {size_str:>8} | {path}")
        
        # 2. –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤
        print(f"\nüìÑ –ê–ù–ê–õ–ò–ó –¢–ò–ü–û–í –§–ê–ô–õ–û–í:")
        content = results['content']
        
        for ext, info in sorted(content['file_types'].items(), key=lambda x: x[1]['total_size'], reverse=True):
            print(f"\n   –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {ext if ext else '(–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)'}")
            print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {info['count']:,}")
            print(f"   ‚Ä¢ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {self._format_size(info['total_size'])}")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {self._format_size(info['avg_size'])}")
            if info['largest']:
                print(f"   ‚Ä¢ –°–∞–º—ã–π –±–æ–ª—å—à–æ–π: {self._format_size(info['largest']['size'])} ({info['largest']['name']})")
        
        # 3. –°–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã
        print(f"\nüîç –¢–û–ü-10 –°–ê–ú–´–• –ë–û–õ–¨–®–ò–• –§–ê–ô–õ–û–í:")
        for i, file_info in enumerate(content['largest_files'][:10], 1):
            size_str = self._format_size(file_info['size'])
            print(f"   {i:2d}. {size_str:>10} | {file_info['name']}")
            print(f"       üìÅ {file_info['path'].parent}")
        
        # 4. –ü—Ä–∏–º–µ—Ä—ã —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        print(f"\nüìñ –ê–ù–ê–õ–ò–ó –°–û–î–ï–†–ñ–ò–ú–û–ì–û (–æ–±—Ä–∞–∑—Ü—ã):")
        for sample in content['sample_contents'][:5]:
            print(f"\n   üìÑ –§–∞–π–ª: {Path(sample['file']).name}")
            print(f"   üìè –†–∞–∑–º–µ—Ä: {self._format_size(sample['size'])}")
            
            content_info = sample['content']
            print(f"   üîç –¢–∏–ø: {content_info.get('type', 'unknown')}")
            
            if 'parquet_info' in content_info:
                pinfo = content_info['parquet_info']
                print(f"   üìä Parquet:")
                print(f"      ‚Ä¢ –°—Ç—Ä–æ–∫: {pinfo['num_rows']:,}")
                print(f"      ‚Ä¢ –ö–æ–ª–æ–Ω–æ–∫: {pinfo['num_columns']}")
                print(f"      ‚Ä¢ Row groups: {pinfo['num_row_groups']}")
                print(f"      ‚Ä¢ –°–∂–∞—Ç—ã–π —Ä–∞–∑–º–µ—Ä: {self._format_size(pinfo['compressed_size'])}")
            
            elif 'preview' in content_info:
                print(f"   üìù –ü—Ä–µ–≤—å—é: {content_info['preview']}")
        
        # 5. –î—É–±–ª–∏–∫–∞—Ç—ã
        print(f"\nüóëÔ∏è –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í:")
        duplicates = results['duplicates']
        
        if duplicates['identical_files']:
            total_waste = sum(dup['waste_space'] for dup in duplicates['identical_files'])
            print(f"   ‚ùå –ù–∞–π–¥–µ–Ω—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Ñ–∞–π–ª—ã!")
            print(f"   üíæ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è: {self._format_size(total_waste)}")
            
            for dup in duplicates['identical_files'][:3]:
                print(f"      ‚Ä¢ {dup['count']} —Ñ–∞–π–ª–æ–≤ –ø–æ {self._format_size(dup['size'])}")
        else:
            print(f"   ‚úÖ –ò–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        if duplicates['empty_files']:
            print(f"   üì≠ –ü—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(duplicates['empty_files'])}")
        
        # 6. –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        print(f"\n‚è∞ –í–†–ï–ú–ï–ù–ù–û–ï –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï:")
        time_analysis = results['time_analysis']
        
        if time_analysis['oldest_file'] and time_analysis['newest_file']:
            oldest = time_analysis['oldest_file']
            newest = time_analysis['newest_file']
            
            print(f"   üìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö:")
            print(f"      ‚Ä¢ –°–∞–º—ã–π —Å—Ç–∞—Ä—ã–π: {oldest['time']} ({self._format_size(oldest['size'])})")
            print(f"      ‚Ä¢ –°–∞–º—ã–π –Ω–æ–≤—ã–π: {newest['time']} ({self._format_size(newest['size'])})")
            
            span = newest['time'] - oldest['time']
            print(f"      ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω: {span.days} –¥–Ω–µ–π, {span.seconds // 3600} —á–∞—Å–æ–≤")
        
        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —á–∞—Å–∞–º
        hourly = time_analysis['files_by_hour']
        if hourly:
            busiest_hours = sorted(hourly.items(), key=lambda x: x[1], reverse=True)[:3]
            print(f"   üïê –°–∞–º—ã–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —á–∞—Å—ã:")
            for hour, count in busiest_hours:
                print(f"      ‚Ä¢ {hour:02d}:00 - {count:,} —Ñ–∞–π–ª–æ–≤")
        
        # 7. –ê–Ω–æ–º–∞–ª–∏–∏
        print(f"\nüö® –ù–ê–ô–î–ï–ù–ù–´–ï –ê–ù–û–ú–ê–õ–ò–ò:")
        anomalies = results['anomalies']
        
        if anomalies['oversized_files']:
            print(f"   üìà –ê–Ω–æ–º–∞–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã:")
            for anom in anomalies['oversized_files'][:5]:
                print(f"      ‚Ä¢ {Path(anom['file']).name}: {self._format_size(anom['size'])} (–æ–∂–∏–¥–∞–ª–æ—Å—å < {anom['expected_max']})")
        
        if anomalies['undersized_files']:
            print(f"   üìâ –ê–Ω–æ–º–∞–ª—å–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã: {len(anomalies['undersized_files'])}")
        
        if anomalies['suspicious_names']:
            print(f"   ü§î –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞: {len(anomalies['suspicious_names'])}")
        
        # 8. –í–´–í–û–î–´
        print(f"\n" + "="*80)
        print("üéØ –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
        print("="*80)
        
        self._generate_conclusions(results)
    
    def _generate_conclusions(self, results):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        content = results['content']
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Å—Ç–∞
        total_data_files = 0
        total_meta_files = 0
        
        for ext, info in content['file_types'].items():
            if ext in ['.1', '.parquet']:
                total_data_files += info['total_size']
            elif ext == '.meta':
                total_meta_files += info['total_size']
        
        print(f"üìä –†–ï–ê–õ–¨–ù–û–ï –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ú–ï–°–¢–ê:")
        print(f"   ‚Ä¢ –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö (.1): {self._format_size(total_data_files)} ({total_data_files/1024**3:.1f} GB)")
        print(f"   ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (.meta): {self._format_size(total_meta_files)} ({total_meta_files/1024**2:.1f} MB)")
        
        # –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        if total_data_files > 0:
            data_count = content['file_types'].get('.1', {}).get('count', 0)
            avg_data_file = total_data_files / max(data_count, 1)
            
            print(f"\nüîç –ê–ù–ê–õ–ò–ó –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò:")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö: {self._format_size(avg_data_file)}")
            
            if avg_data_file < 10 * 1024 * 1024:  # < 10MB
                print(f"   ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –§–∞–π–ª—ã —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ!")
                print(f"   üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –£–≤–µ–ª–∏—á–∏—Ç—å batch_size –≤ 10-100 —Ä–∞–∑")
            
            # –û—Ü–µ–Ω–∫–∞ —Å–∂–∞—Ç–∏—è
            estimated_uncompressed = total_data_files * 3  # –ü—Ä–∏–º–µ—Ä–Ω–æ 3x —Å–∂–∞—Ç–∏–µ –¥–ª—è Parquet
            print(f"   üì¶ –û—Ü–µ–Ω–∫–∞ —Å–∂–∞—Ç–∏—è: {self._format_size(estimated_uncompressed)} ‚Üí {self._format_size(total_data_files)}")
            print(f"   üìà –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: ~{estimated_uncompressed/max(total_data_files, 1):.1f}x")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö
        print(f"\nü§î –ü–†–û–í–ï–†–ö–ê –†–ê–ó–£–ú–ù–û–°–¢–ò –û–ë–™–ï–ú–ê:")
        
        # –î–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        time_analysis = results['time_analysis']
        if time_analysis['oldest_file'] and time_analysis['newest_file']:
            oldest = time_analysis['oldest_file']['time']
            newest = time_analysis['newest_file']['time']
            days = (newest - oldest).days + 1
            
            gb_per_day = total_data_files / 1024**3 / days
            print(f"   üìà –î–∞–Ω–Ω—ã—Ö –≤ –¥–µ–Ω—å: {gb_per_day:.2f} GB")
            print(f"   üìä –ó–∞ {days} –¥–Ω–µ–π: {total_data_files/1024**3:.1f} GB")
            
            # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è BTC trades
            if gb_per_day > 50:
                print(f"   ‚ö†Ô∏è –ú–ù–û–ì–û: {gb_per_day:.1f} GB/–¥–µ–Ω—å –∫–∞–∂–µ—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω—ã–º –¥–ª—è –æ–¥–Ω–æ–π –ø–∞—Ä—ã BTC")
                print(f"   üîç –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                print(f"      ‚Ä¢ –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
                print(f"      ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
                print(f"      ‚Ä¢ –ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ")
            elif gb_per_day < 0.1:
                print(f"   ü§î –ú–ê–õ–û: {gb_per_day:.3f} GB/–¥–µ–Ω—å –∫–∞–∂–µ—Ç—Å—è –º–∞–ª–æ –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ BTC")
            else:
                print(f"   ‚úÖ –ù–û–†–ú–ê–õ–¨–ù–û: {gb_per_day:.1f} GB/–¥–µ–Ω—å –≤—ã–≥–ª—è–¥–∏—Ç —Ä–∞–∑—É–º–Ω–æ")

def main():
    """–ó–∞–ø—É—Å–∫ –≥–ª—É–±–æ–∫–æ–π –∏–Ω—Å–ø–µ–∫—Ü–∏–∏"""
    print("üî¨ –ì–õ–£–ë–û–ö–ê–Ø –ò–ù–°–ü–ï–ö–¶–ò–Ø –î–ê–ù–ù–´–•")
    print("–í—ã—è—Å–Ω—è–µ–º –ß–¢–û –∏–º–µ–Ω–Ω–æ –∑–∞–Ω–∏–º–∞–µ—Ç 101GB –º–µ—Å—Ç–∞")
    print("="*60)
    
    minio_path = Path("data/minio")
    if not minio_path.exists():
        print(f"‚ùå –ü—É—Ç—å {minio_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    inspector = DeepDataInspector(str(minio_path))
    
    try:
        inspector.full_inspection()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Å–ø–µ–∫—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()